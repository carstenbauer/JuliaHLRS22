{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, [BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl) provides the necessary tools to micro-benchmark a certain piece of Julia code. However, sometimes we want to zoom out and identify bottlenecks in a larger block of code.\n",
    "\n",
    "There are two different techniques that we'll use:\n",
    "* **Instrumented** profiling\n",
    "* **Statistical** profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Matrix-Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matmul (generic function with 2 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function matmul(n, k=n)\n",
    "    A = rand(n, k)\n",
    "    B = rand(k, n)\n",
    "    C = zeros(n, n)\n",
    "    # simple matmul implementation\n",
    "    for n in axes(C, 2), m in axes(C, 1)\n",
    "        Cmn = zero(eltype(C))\n",
    "        for k in axes(A, 2)\n",
    "            tmp = A[m, k] * B[k, n]\n",
    "            Cmn += tmp\n",
    "        end\n",
    "        C[m, n] = Cmn\n",
    "    end\n",
    "    return C\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul(10, 5); # trigger compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumented Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to modify our code and explicitly add profiling bits to it. Specifically, we'll use [TimerOutputs.jl](https://github.com/KristofferC/TimerOutputs.jl).\n",
    "\n",
    "**Pros**\n",
    "* Accurate and complete performance statistics\n",
    "\n",
    "**Cons**\n",
    "* Need to modify the source code\n",
    "* Some overhead\n",
    "* Limited support for multithreading ([TrackingTimers.jl](https://github.com/ericphanson/TrackingTimers.jl) may be an alternative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TimerOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matmul_instrumented (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function matmul_instrumented(n, k=n)\n",
    "    @timeit \"initialize matrices\" begin\n",
    "        @timeit \"init A\" A = rand(n, k)\n",
    "        @timeit \"init B\" B = rand(k, n)\n",
    "        @timeit \"init C\" C = zeros(n, n)\n",
    "    end\n",
    "    # simple matmul implementation\n",
    "    @timeit \"matmul\" for n in axes(C, 2), m in axes(C, 1)\n",
    "        Cmn = zero(eltype(C))\n",
    "        for k in axes(A, 2)\n",
    "            @timeit \"mul\" tmp = A[m, k] * B[k, n]\n",
    "            @timeit \"add\" Cmn += tmp\n",
    "        end\n",
    "        C[m, n] = Cmn\n",
    "    end\n",
    "    return C\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m\n",
       "\u001b[0m\u001b[1m                               \u001b[22m         Time                    Allocations      \n",
       "                               ───────────────────────   ────────────────────────\n",
       "       Tot / % measured:            39.8h /   0.0%            852MiB /   0.0%    \n",
       "\n",
       " Section               ncalls     time    %tot     avg     alloc    %tot      avg\n",
       " ────────────────────────────────────────────────────────────────────────────────\n",
       " matmul                     2    104ms  100.0%  52.2ms   1.47KiB    0.8%     752B\n",
       "   mul                   200k   21.3ms   20.4%   106ns     0.00B    0.0%    0.00B\n",
       "   add                   200k   20.2ms   19.4%   101ns     0.00B    0.0%    0.00B\n",
       " initialize matrices        2   32.9μs    0.0%  16.4μs    190KiB   99.2%  95.1KiB\n",
       "   init C                   2   11.4μs    0.0%  5.72μs    156KiB   81.5%  78.2KiB\n",
       "   init A                   2   9.91μs    0.0%  4.96μs   15.9KiB    8.3%  7.94KiB\n",
       "   init B                   2   4.38μs    0.0%  2.19μs   15.9KiB    8.3%  7.94KiB\n",
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to = TimerOutputs.get_defaulttimer()\n",
    "# TimerOutputs.reset_timer!(to)\n",
    "matmul_instrumented(100, 10);\n",
    "to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────\u001b[22m\n",
       "\u001b[0m\u001b[1m                   \u001b[22m         Time                    Allocations      \n",
       "                   ───────────────────────   ────────────────────────\n",
       " Tot / % measured:      268μs /   0.0%           3.99KiB /   0.0%    \n",
       "\n",
       " Section   ncalls     time    %tot     avg     alloc    %tot      avg\n",
       " ────────────────────────────────────────────────────────────────────\n",
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────\u001b[22m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to = TimerOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────\u001b[22m\n",
       "\u001b[0m\u001b[1m                   \u001b[22m         Time                    Allocations      \n",
       "                   ───────────────────────   ────────────────────────\n",
       " Tot / % measured:      5.48s /   0.0%            179KiB /   0.0%    \n",
       "\n",
       " Section   ncalls     time    %tot     avg     alloc    %tot      avg\n",
       " ────────────────────────────────────────────────────────────────────\n",
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────\u001b[22m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_instrumented(100, 10);\n",
    "to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to repeatedly record the state of the program (i.e. which line or function is currently executing) while it is running with a given sample rate.\n",
    "\n",
    "Julia has built-in [statistical profilers](https://goo.gl/Ycz4Td) in the standard library [`Profile`](https://docs.julialang.org/en/v1/stdlib/Profile/) (see also [here](https://docs.julialang.org/en/v1/manual/profile/)). We will use these profilers to identify the parts of our `matmul` function that have\n",
    "* the highest computation time\n",
    "* make the most / the biggest allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling is as simple as prepending the function call by the `@profile` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Profile\n",
    "Profile.clear() # clean up old profiling data\n",
    "@profile matmul(1000, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way to analyze the profiling results is `Profile.print()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count  Overhead File                    Line Function\n",
      " =====  ======== ====                    ==== ========\n",
      "    66         0 In[1]                      9 matmul(n::Int64, k::Int64)\n",
      "     4         0 In[1]                     10 matmul(n::Int64, k::Int64)\n",
      "    52         0 In[1]                     11 matmul(n::Int64, k::Int64)\n",
      "    44         0 @Base/array.jl           925 getindex\n",
      "   124         0 @Base/boot.jl            368 eval\n",
      "   124         0 @Base/essentials.jl      729 #invokelatest#2\n",
      "   124         0 @Base/essentials.jl      726 invokelatest\n",
      "    22         0 @Base/float.jl           385 *\n",
      "     4         0 @Base/float.jl           383 +\n",
      "   124         0 @Base/loading.jl        1428 include_string(mapexpr::typeof(...\n",
      "   124       124 @Base/task.jl            484 (::IJulia.var\"#15#18\")()\n",
      "   124         0 ...ia/src/eventloop.jl     8 eventloop(socket::ZMQ.Socket)\n",
      "   124         0 .../execute_request.jl    67 execute_request(socket::ZMQ.Soc...\n",
      "   124         0 .../SoftGlobalScope.jl    65 softscope_include_string(m::Mod...\n",
      "Total snapshots: 127 (100% utilization across all threads and tasks. Use the `groupby` kwarg to break down by thread and/or task)\n"
     ]
    }
   ],
   "source": [
    "Profile.print(; threads=1, format=:flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much nicer way to analyze the profiling results is to visualize them as a flame graph. In principle, one can choose from a number of visualization tools. To name a few:\n",
    "\n",
    "* [ProfileView.jl](https://github.com/timholy/ProfileView.jl)\n",
    "* [ProfileVega.jl](https://github.com/davidanthoff/ProfileVega.jl)\n",
    "* [ProfileSVG.jl](https://github.com/kimikage/ProfileSVG.jl)\n",
    "* [PProf.jl](https://github.com/JuliaPerf/PProf.jl)\n",
    "* ...\n",
    "\n",
    "However, personally, I recommend to use the [Julia extension for Visual Studio Code (VS Code)](https://www.julia-vscode.org/) which has built-in [profiling visualization capabilities](https://www.julia-vscode.org/docs/stable/userguide/profiler/). Let's take a closer look..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Hardware-Level Performance Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have considered **software** profiling options. Another approach to assessing the performance of a (piece of) Julia code are **hardware** performance counters, which are built into most modern CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize those counters in Julia, one can use **[LIKWID.jl](https://github.com/JuliaPerf/LIKWID.jl)** which is a wrapper around the performance monitoring and benchmarking suite [LIKWID](https://github.com/RRZE-HPC/likwid) (Like I Knew What I'm Doing) by the [Erlangen National High Performance Computing Center (NHR@FAU)](https://hpc.fau.de/). Conceptually, it provides tools for both instrumented (e.g. marker API) and statistical (e.g. timeline and stethoscope mode) performance monitoring.\n",
    "\n",
    "<div style=\"float\">\n",
    "    <img src=\"../../imgs/likwidjl_logo.png\" width=350px>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    <img src=\"../../imgs/likwid_logo.png\" width=300px>\n",
    "</div>\n",
    "\n",
    "**LIKWID.jl** allows one to obtain detailed low-performance metrics for a (piece of) Julia code to answer questions such as\n",
    "* How many FLOPs have been performed?\n",
    "* What fraction of the FLOPs have been vectorized? (SIMD)\n",
    "* How much data has been read from / written to memory?\n",
    "\n",
    "**Most important commands:**\n",
    "\n",
    "* `PerfMon.supported_groups()`: List all available performance groups (\"what to measure\").\n",
    "  * Examples:\n",
    "    * \"FLOPS_SP\" / \"FLOPS_DP\": single or double precision floating point operations\n",
    "    * \"MEM\": memory related metrics\n",
    "* `@perfmon <performance_group> <code>`\n",
    "  * Example: `@perfmon \"FLOPS_DP\" myfunc(x)`.\n",
    "\n",
    "[Demonstration on the cluster...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information see:\n",
    "* [LIKWID.jl Documentation](https://juliaperf.github.io/LIKWID.jl/dev/)\n",
    "* [JuliaCon2022 Talk (Youtube)](https://www.youtube.com/watch?v=l2fTNfEDPC0)\n",
    "* [LIKWID Wiki](https://github.com/RRZE-HPC/likwid/wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../imgs/likwid_example.png\" width=900px>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
