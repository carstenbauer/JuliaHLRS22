{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing with Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topology of a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/gpu_topology.png\" width=1300px>\n",
    "\n",
    "**Source:** [Sivalingam, Karthee. \"GPU Acceleration of a Theoretical Particle Physics Application.\" Master's Thesis, The University of Edinburgh (2010).](https://static.epcc.ed.ac.uk/dissertations/hpc-msc/2009-2010/Karthee%20Sivalingam.pdf)\n",
    "\n",
    "* **SM** = Streaming Multiprocessor\n",
    "* **SP** = Streaming Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA A100 SXM4\n",
    "\n",
    "<img src=\"../imgs/a100_front.png\" width=800px>\n",
    "\n",
    "<img src=\"../imgs/a100_SM.png\" width=400px>\n",
    "\n",
    "**Source:** [NVIDIA whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\n",
    "\n",
    "| Kind                       | Count            |\n",
    "|----------------------------|------------------|\n",
    "| **SMs**                    | 108              |\n",
    "| **CUDA cores** / FP32 ALUs | 6912 (64 per SM) |\n",
    "| **Tensor cores**           | 432 (4 per SM)   |\n",
    "\n",
    "* **ALU** = Arithmetic Logical Unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GPUInspector\n",
    "# gpuinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JuliaGPU\n",
    "\n",
    "Website: https://juliagpu.org/\n",
    "\n",
    "GitHub Org: https://github.com/JuliaGPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(We'll focus on Nvidia GPUs but there is [support for other GPUs](https://juliagpu.org/) as well.)\n",
    "\n",
    "The interface to NVIDIA GPU computing in Julia is [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl).\n",
    "\n",
    "\n",
    "\n",
    "It provides:\n",
    "\n",
    "* **High-level abstraction `CuArray`**\n",
    "* **Tools for writing custom CUDA kernels**\n",
    "* **Wrappers to proprietary NVIDIA libraries (e.g. CUBLAS, CUFFT, CUSPARSE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA toolkit 11.7, artifact installation\n",
      "NVIDIA driver 510.47.3, for CUDA 11.6\n",
      "CUDA driver 11.7\n",
      "\n",
      "Libraries: \n",
      "- CUBLAS: 11.10.1\n",
      "- CURAND: 10.2.10\n",
      "- CUFFT: 10.7.2\n",
      "- CUSOLVER: 11.3.5\n",
      "- CUSPARSE: 11.7.3\n",
      "- CUPTI: 17.0.0\n",
      "- NVML: 11.0.0+510.47.3\n",
      "- CUDNN: 8.30.2 (for CUDA 11.5.0)\n",
      "- CUTENSOR: 1.4.0 (for CUDA 11.5.0)\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.7.3\n",
      "- LLVM: 12.0.1\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n",
      "- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n",
      "\n",
      "8 devices:\n",
      "  0: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  1: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  2: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  3: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  4: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  5: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  6: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  7: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo() # automatically downloads CUDA framework if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level abstraction: `CuArray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `CuArray` is a CPU handle to GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       "  7.925843f-12\n",
       "  3.7553988\n",
       " -1.0145275f-18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_gpu = CuArray{Float32}(undef, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.20692769\n",
       " 0.71898437\n",
       " 0.28365776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.rand(3) # Note: defaults to Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.zeros(3) # Note: defaults to Float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can readily move data to the GPU by converting to `CuArray`.\n",
    "\n",
    " <img src=\"../imgs/cpu_gpu_transfer.svg\" width=180px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_cpu = [1,2,3]\n",
    "x_gpu = CuArray(x_cpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(or by using `copyto!` to move it into already allocated memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CuArray`s are managed by Julia's **garbage collector**. In principle, if they are unreachable, they get cleaned up automatically.\n",
    "\n",
    "However, by default CUDA.jl uses a **memory pool** to speed up allocations. So it might appear as if the objects have not been free'd. (You can disable the pool with `JULIA_CUDA_MEMORY_POOL=none`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 2.24% (901.938 MiB/39.409 GiB)\n",
      "Memory pool usage: 30.518 MiB (64.000 MiB reserved)"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"38.147 MiB\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Base.format_bytes(sizeof(x_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = nothing; GC.gc(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 2.12% (853.938 MiB/39.409 GiB)\n",
      "Memory pool usage: 19.074 MiB (64.000 MiB reserved)"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `CUDA.unsafe_free!(x_gpu)` and `CUDA.reclaim()` to more aggressively suggest the freeing of the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.unsafe_free!(x_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 2.12% (853.938 MiB/39.409 GiB)\n",
      "Memory pool usage: 19.074 MiB (64.000 MiB reserved)"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CuArray <: AbstractArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we should be able to do all kind of operations with it, that we'd also do with regular `Array`s. (**duck typing**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 0.056945   0.839401    0.813055   …  0.163615   0.114515   0.119574\n",
       " 0.0182427  0.00105949  0.594533      0.234975   0.69579    0.788363\n",
       " 0.473413   0.30368     0.273531      0.885249   0.402535   0.915592\n",
       " 0.903833   0.688429    0.982785      0.944259   0.897768   0.711426\n",
       " 0.835315   0.305328    0.764077      0.751928   0.0361518  0.170335\n",
       " 0.617305   0.710764    0.150943   …  0.268591   0.665839   0.623263\n",
       " 0.74807    0.0914348   0.0219869     0.0830913  0.159481   0.623509\n",
       " 0.668722   0.0886      0.119556      0.770323   0.742287   0.2565\n",
       " 0.945929   0.0817244   0.924021      0.220601   0.913536   0.139473\n",
       " 0.181236   0.378013    0.247321      0.903421   0.647854   0.421166\n",
       " ⋮                                 ⋱                        \n",
       " 0.339104   0.553526    0.300516      0.381663   0.243767   0.940985\n",
       " 0.11222    0.545936    0.0460838     0.23775    0.505224   0.0717938\n",
       " 0.295375   0.535546    0.0783167     0.35869    0.371998   0.354804\n",
       " 0.918197   0.270604    0.888399      0.119653   0.47988    0.790812\n",
       " 0.467912   0.857779    0.339838   …  0.85344    0.798594   0.459207\n",
       " 0.915391   0.427648    0.856715      0.401663   0.869323   0.899331\n",
       " 0.43174    0.510017    0.899353      0.272091   0.848915   0.223763\n",
       " 0.160494   0.323498    0.77908       0.569486   0.860064   0.548859\n",
       " 0.0869596  0.105198    0.45429       0.956127   0.380432   0.509668"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 1000\n",
    "A_gpu = CUDA.rand(N,N)\n",
    "B_gpu = CUDA.rand(N,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 245.87   246.475  250.683  256.683  …  252.145  252.805  250.717  258.358\n",
       " 236.848  241.92   239.251  246.927     246.033  249.34   242.422  252.599\n",
       " 240.154  237.077  235.73   250.582     243.024  246.821  246.604  253.176\n",
       " 253.03   255.178  247.617  258.726     252.536  255.284  256.25   263.432\n",
       " 242.494  235.547  238.986  244.049     249.6    244.453  244.351  251.282\n",
       " 240.303  234.647  242.09   249.004  …  244.226  242.981  242.436  249.887\n",
       " 241.933  243.031  245.059  255.049     251.519  247.378  249.662  257.829\n",
       " 242.441  242.162  240.999  245.55      244.424  246.555  244.995  257.67\n",
       " 248.442  247.131  249.131  250.933     254.358  248.887  253.567  260.742\n",
       " 243.909  243.63   251.266  256.549     254.509  251.602  252.063  257.38\n",
       "   ⋮                                 ⋱                             \n",
       " 249.256  243.497  250.247  250.393     251.865  252.332  255.247  258.112\n",
       " 252.923  246.637  244.23   254.123     251.577  250.088  255.404  253.941\n",
       " 242.183  241.232  242.152  255.545     251.972  247.468  254.793  254.273\n",
       " 249.03   245.301  246.159  256.009     253.994  249.069  254.437  258.628\n",
       " 239.737  238.316  241.521  244.494  …  244.84   247.017  251.215  255.752\n",
       " 262.584  262.965  260.498  269.874     262.895  267.57   265.06   270.164\n",
       " 238.567  238.471  236.444  246.427     246.65   247.729  247.605  249.564\n",
       " 243.028  244.84   241.202  249.539     249.937  244.316  251.314  254.24\n",
       " 249.202  243.795  242.272  249.818     254.652  247.706  253.137  257.138"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_gpu * B_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.468 ms (2 allocations: 3.81 MiB)\n",
      "  17.814 μs (29 allocations: 592 bytes)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime A_cpu * B_cpu setup=(A_cpu = rand(Float32, N,N); B_cpu = rand(Float32, N,N););\n",
    "@btime A_gpu * B_gpu setup=(A_gpu = CUDA.rand(N,N); B_gpu = CUDA.rand(N,N););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the timescales change: **milliseconds -> microseconds**!\n",
    "\n",
    "(`*` for `CuArray`s uses a cuBLAS kernel under the hood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 1.23208   0.944068  1.38154   1.08824   …  0.87209   0.672663  1.58735\n",
       " 0.984475  1.5205    0.716376  1.15109      1.27782   1.61814   1.07899\n",
       " 0.942673  0.548932  1.47714   1.54724      0.902197  1.20323   0.834586\n",
       " 0.497924  0.712583  1.22956   0.885402     1.36429   1.54674   0.79038\n",
       " 0.871517  1.31615   1.4691    1.55913      1.39146   0.750631  1.02665\n",
       " 0.394154  1.30235   0.922221  1.75687   …  1.2703    0.493146  0.447018\n",
       " 1.22646   1.1364    0.387659  0.527573     0.77348   0.510331  1.03629\n",
       " 1.63194   1.04339   1.32871   0.530629     1.15481   1.22      0.865854\n",
       " 1.07494   0.596293  0.566374  0.423238     0.791183  1.80143   0.859054\n",
       " 0.640312  1.16007   1.23231   1.56074      1.21121   1.71157   0.88117\n",
       " ⋮                                       ⋱                      \n",
       " 0.616944  0.325943  0.985325  0.959406     1.09151   1.54474   1.78911\n",
       " 1.1108    1.27737   1.51201   0.923711     1.06877   0.79366   0.88223\n",
       " 0.732747  1.02296   0.172119  0.82142      1.47947   1.20906   1.29646\n",
       " 0.706586  1.34349   0.961991  0.404567     0.851877  1.43973   0.851175\n",
       " 1.19889   1.55857   0.851588  0.803748  …  1.10677   1.35831   1.09686\n",
       " 0.471754  1.04402   1.0018    0.122114     0.684822  0.88524   0.778288\n",
       " 1.49147   0.949639  1.34796   0.551157     0.413203  1.18787   1.41215\n",
       " 0.745183  1.53592   0.445941  1.12914      0.81354   1.02525   0.73848\n",
       " 0.337061  0.693497  0.936463  0.816169     1.42361   1.06079   1.25016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_gpu .+ B_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Broadcasting, `map`, `reduce`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 1.23208   0.944068  1.38154   1.08824   …  0.87209   0.672663  1.58735\n",
       " 0.984475  1.5205    0.716376  1.15109      1.27782   1.61814   1.07899\n",
       " 0.942673  0.548932  1.47714   1.54724      0.902197  1.20323   0.834586\n",
       " 0.497924  0.712583  1.22956   0.885402     1.36429   1.54674   0.79038\n",
       " 0.871517  1.31615   1.4691    1.55913      1.39146   0.750631  1.02665\n",
       " 0.394154  1.30235   0.922221  1.75687   …  1.2703    0.493146  0.447018\n",
       " 1.22646   1.1364    0.387659  0.527573     0.77348   0.510331  1.03629\n",
       " 1.63194   1.04339   1.32871   0.530629     1.15481   1.22      0.865854\n",
       " 1.07494   0.596293  0.566374  0.423238     0.791183  1.80143   0.859054\n",
       " 0.640312  1.16007   1.23231   1.56074      1.21121   1.71157   0.88117\n",
       " ⋮                                       ⋱                      \n",
       " 0.616944  0.325943  0.985325  0.959406     1.09151   1.54474   1.78911\n",
       " 1.1108    1.27737   1.51201   0.923711     1.06877   0.79366   0.88223\n",
       " 0.732747  1.02296   0.172119  0.82142      1.47947   1.20906   1.29646\n",
       " 0.706586  1.34349   0.961991  0.404567     0.851877  1.43973   0.851175\n",
       " 1.19889   1.55857   0.851588  0.803748  …  1.10677   1.35831   1.09686\n",
       " 0.471754  1.04402   1.0018    0.122114     0.684822  0.88524   0.778288\n",
       " 1.49147   0.949639  1.34796   0.551157     0.413203  1.18787   1.41215\n",
       " 0.745183  1.53592   0.445941  1.12914      0.81354   1.02525   0.73848\n",
       " 0.337061  0.693497  0.936463  0.816169     1.42361   1.06079   1.25016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_gpu .+ B_gpu # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 0.882729  0.905507  1.04831   0.919461   …  0.624264  0.53254   1.13817\n",
       " 0.975172  1.10301   0.70086   0.819059      0.904389  1.1475    0.772294\n",
       " 0.721972  0.445355  1.066     1.10104       0.776077  0.953321  0.762243\n",
       " 0.467867  0.678417  0.880172  0.869848      1.0625    1.09393   0.620383\n",
       " 0.753092  1.04051   1.03993   1.10247       1.06317   0.64698   0.804795\n",
       " 0.379931  0.988231  0.696314  1.25082    …  0.923021  0.482442  0.322914\n",
       " 1.02472   0.925946  0.287411  0.382556      0.642757  0.424038  0.981718\n",
       " 1.16397   0.811652  1.0435    0.455143      0.889739  0.991057  0.703174\n",
       " 0.764239  0.519799  0.566025  0.381462      0.56915   1.27814   0.622201\n",
       " 0.499043  0.894359  0.990353  1.11118       0.878017  1.2255    0.816925\n",
       " ⋮                                        ⋱                      \n",
       " 0.500579  0.289495  0.945863  0.695202      0.993257  1.09441   1.26558\n",
       " 0.78599   0.942759  1.08715   0.653199      0.755846  0.565297  0.799878\n",
       " 0.531075  0.814878  0.144725  0.690767      1.04648   1.01636   0.981653\n",
       " 0.561496  0.962191  0.802047  0.317172      0.697605  1.01886   0.637561\n",
       " 0.89213   1.12432   0.698473  0.794918   …  0.803535  0.986586  0.859358\n",
       " 0.41512   0.994011  0.744486  0.0998844     0.491208  0.627248  0.720052\n",
       " 1.06596   0.672431  1.05346   0.487602      0.373274  0.898854  1.07779\n",
       " 0.534139  1.08953   0.315368  0.862623      0.613763  0.844801  0.528719\n",
       " 0.239136  0.530134  0.759752  0.600876      1.04543   0.773717  0.89017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqrt.(A_gpu.^2 + B_gpu.^2) # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459653.28f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapreduce(sin, +, A_gpu) # runs on the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Counter-example:\" Scalar indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore are only permitted from the REPL for prototyping purposes.\n",
      "If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base ./error.jl:33\n",
      " [2] assertscalar(op::String)\n",
      "   @ GPUArraysCore /scratch/pc2-mitarbeiter/bauerc/.julia/packages/GPUArraysCore/lojQM/src/GPUArraysCore.jl:87\n",
      " [3] getindex(xs::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, I::Int64)\n",
      "   @ GPUArrays /scratch/pc2-mitarbeiter/bauerc/.julia/packages/GPUArrays/fqD8z/src/host/indexing.jl:9\n",
      " [4] top-level scope\n",
      "   @ /scratch/pc2-mitarbeiter/bauerc/teaching/JuliaHLRS22/Day4/1_gpu_computing.ipynb:1"
     ]
    }
   ],
   "source": [
    "A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5155314f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.@allowscalar A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu_broadcasting! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function gpu_not_actually!(C, A, B)\n",
    "    CUDA.@allowscalar for i in eachindex(A,B)\n",
    "        C[i] = A[i] * B[i]\n",
    "    end\n",
    "end\n",
    "\n",
    "function gpu_broadcasting!(C, A, B)\n",
    "    C .= A .* B\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.834 ms (901 allocations: 140.64 KiB)\n",
      "  4.181 μs (11 allocations: 576 bytes)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "N = 10\n",
    "@btime gpu_not_actually!(C, A, B) setup=(A = CUDA.rand(10,10); B = CUDA.rand(10,10); C = CUDA.rand(10,10););\n",
    "@btime gpu_broadcasting!(C, A, B) setup=(A = CUDA.rand(10,10); B = CUDA.rand(10,10); C = CUDA.rand(10,10););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FLoops: CUDA executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu_floops! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using FLoops, FoldsCUDA\n",
    "\n",
    "function gpu_floops!(C, A, B)\n",
    "    @floop CUDAEx() for i in eachindex(A,B,C)\n",
    "        C[i] = A[i] * B[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  40.577 μs (237 allocations: 19.67 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime gpu_floops!(C, A, B) setup=(A = CUDA.rand(10,10); B = CUDA.rand(10,10); C = CUDA.rand(10,10););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel programming: Writing CUDA kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CUDA kernel is a function that will be executed by all GPU *threads* separately. Based on the index of a thread we can make them operate on different pieces of given data (Single Program Multiple Data (SPMD) programming model similar to MPI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuda_kernel! (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = CUDA.zeros(1024)\n",
    "\n",
    "function cuda_kernel!(x)\n",
    "    i = threadIdx().x\n",
    "    x[i] += 1\n",
    "    return nothing # CUDA kernels should never return anything\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(cuda_kernel!), Tuple{CuDeviceVector{Float32, 1}}}(cuda_kernel!, CuFunction(Ptr{Nothing} @0x000000000b751240, CuModule(Ptr{Nothing} @0x000000000b7e6ad0, CuContext(0x0000000006f118a0, instance 8462ee47003f6eed))), CUDA.KernelState(Ptr{Nothing} @0x00007f2642400000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(cuda_kernel!), Tuple{CuDeviceVector{Float32, 1}}}(cuda_kernel!, CuFunction(Ptr{Nothing} @0x000000000b751240, CuModule(Ptr{Nothing} @0x000000000b7e6ad0, CuContext(0x0000000006f118a0, instance 8462ee47003f6eed))), CUDA.KernelState(Ptr{Nothing} @0x00007f2642400000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " ⋮\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel programming can quickly become (much) more difficult though because\n",
    "* you need to respect **hardware limitations** of the GPU\n",
    "* **not all operations can readily be expressed as scalar kernels** (e.g. reductions)\n",
    "* since kernels execute on the GPU, the Julia runtime isn't available and kernel code has limitations (**you can't just write arbitrary Julia code in kernels**)\n",
    "  * no GC / no allocations\n",
    "  * must be fully type inferred\n",
    "  * no `try ... catch ... end`\n",
    "  * no strings\n",
    "  * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example for a hardware limitation: **A100 supports a maximal number of 1024 threads.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "CuError",
     "evalue": "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
     "output_type": "error",
     "traceback": [
      "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] throw_api_error(res::CUDA.cudaError_enum)\n",
      "    @ CUDA /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/error.jl:89\n",
      "  [2] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/error.jl:97 [inlined]\n",
      "  [3] cuLaunchKernel\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/utils/call.jl:26 [inlined]\n",
      "  [4] (::CUDA.var\"#39#40\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3})(kernelParams::Vector{Ptr{Nothing}})\n",
      "    @ CUDA /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:69\n",
      "  [5] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:33 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ ./none:0 [inlined]\n",
      "  [7] pack_arguments(::CUDA.var\"#39#40\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3}, ::CUDA.KernelState, ::CuDeviceVector{Float32, 1})\n",
      "    @ CUDA ./none:0\n",
      "  [8] #launch#38\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:62 [inlined]\n",
      "  [9] #44\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:136 [inlined]\n",
      " [10] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:95 [inlined]\n",
      " [11] macro expansion\n",
      "    @ ./none:0 [inlined]\n",
      " [12] convert_arguments\n",
      "    @ ./none:0 [inlined]\n",
      " [13] #cudacall#43\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:135 [inlined]\n",
      " [14] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:204 [inlined]\n",
      " [15] macro expansion\n",
      "    @ ./none:0 [inlined]\n",
      " [16] #call#207\n",
      "    @ ./none:0 [inlined]\n",
      " [17] (::CUDA.HostKernel{typeof(cuda_kernel!), Tuple{CuDeviceVector{Float32, 1}}})(args::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}; threads::Int64, blocks::Int64, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})\n",
      "    @ CUDA /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:484\n",
      " [18] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:104 [inlined]\n",
      " [19] top-level scope\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/src/utilities.jl:25"
     ]
    }
   ],
   "source": [
    "x = CUDA.zeros(1025)\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What if we want to go larger?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/cuda_blocks_threads.png\" width=700px>\n",
    "\n",
    "(Note: in Julia indices start at 1)\n",
    "\n",
    "**Source:** [Sivalingam, Karthee. \"GPU Acceleration of a Theoretical Particle Physics Application.\" Master's Thesis, The University of Edinburgh (2010).](https://static.epcc.ed.ac.uk/dissertations/hpc-msc/2009-2010/Karthee%20Sivalingam.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuda_kernel_blocks! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cuda_kernel_blocks!(x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(x)\n",
    "        @inbounds x[i] += 1\n",
    "    end\n",
    "    return nothing # CUDA kernels should never return anything\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(cuda_kernel_blocks!), Tuple{CuDeviceVector{Float32, 1}}}(cuda_kernel_blocks!, CuFunction(Ptr{Nothing} @0x0000000009150f20, CuModule(Ptr{Nothing} @0x000000000b793d90, CuContext(0x0000000006f118a0, instance 8462ee47003f6eed))), CUDA.KernelState(Ptr{Nothing} @0x00007f2642400000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = CUDA.zeros(1025)\n",
    "CUDA.@sync @cuda threads=1024 blocks=2 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ⋮\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our custom CUDA kernel compare to broadcasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12.724 μs (5 allocations: 304 bytes)\n",
      "  15.779 μs (30 allocations: 1.58 KiB)\n"
     ]
    }
   ],
   "source": [
    "function launch_kernel(x)\n",
    "    CUDA.@sync @cuda threads=1024 blocks=1 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "@btime launch_kernel(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime CUDA.@sync(x .+ 1) setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Three ways to SAXPY on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAXPY** = **S**ingle precision **A** times **X** **P**lus **Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_broadcast_cpu!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Computes the SAXPY on the CPU using broadcasting\"\n",
    "function saxpy_broadcast_cpu!(a, x, y)\n",
    "    y .= a .* x .+ y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_broadcast_gpu!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Computes the SAXPY on the GPU using broadcasting\"\n",
    "function saxpy_broadcast_gpu!(a, x, y)\n",
    "    CUDA.@sync y .= a .* x .+ y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_cuda_kernel!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"CUDA kernel for computing the SAXPY on the GPU\"\n",
    "function _saxpy_kernel!(a, x, y)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(y)\n",
    "        @inbounds y[i] = a * x[i] + y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\"Computes the SAXPY on the GPU using the custom CUDA kernel `_saxpy_kernel!`\"\n",
    "function saxpy_cuda_kernel!(a, x, y; nthreads, nblocks)\n",
    "    CUDA.@sync @cuda(\n",
    "        threads = nthreads,\n",
    "        blocks = nblocks,\n",
    "        _saxpy_kernel!(a, x, y)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_cublas! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function saxpy_cublas!(a, x, y)\n",
    "    CUDA.@sync CUBLAS.axpy!(length(x), a, x, y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using PrettyTables\n",
    "\n",
    "\"Computes the GFLOP/s from the vector length `len` and the measured runtime `t`.\"\n",
    "saxpy_flops(t; len) = 2.0 * len * 1e-9 / t # GFLOP/s\n",
    "\n",
    "\"Computes the GB/s from the vector length `len`, the vector element type `dtype`, and the measured runtime `t`.\"\n",
    "saxpy_bandwidth(t; dtype, len) = 3.0 * sizeof(dtype) * len * 1e-9 / t # GB/s\n",
    "\n",
    "function main()\n",
    "    if !contains(lowercase(name(device())), \"a100\")\n",
    "        @warn(\"This script was tuned for a NVIDIA A100 GPU. Your GPU: $(name(device())).\")\n",
    "    end\n",
    "    dtype = Float32\n",
    "    nthreads = 1024 # CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK\n",
    "    nblocks = 500_000\n",
    "    len = nthreads * nblocks # vector length\n",
    "    a = convert(dtype, 3.1415)\n",
    "    x = ones(dtype, len)\n",
    "    y = ones(dtype, len)\n",
    "    xgpu = CUDA.ones(dtype, len)\n",
    "    ygpu = CUDA.ones(dtype, len)\n",
    "\n",
    "    t_broadcast_cpu = @belapsed saxpy_broadcast_cpu!($a, $x, $y) samples = 10 evals = 2\n",
    "    t_broadcast_gpu = @belapsed saxpy_broadcast_gpu!($a, $xgpu, $ygpu) samples = 10 evals = 2\n",
    "    t_cuda_kernel = @belapsed saxpy_cuda_kernel!($a, $xgpu, $ygpu; nthreads=$nthreads, nblocks=$nblocks) samples = 10 evals = 2\n",
    "    t_cublas = @belapsed saxpy_cublas!($a, $xgpu, $ygpu) samples = 10 evals = 2\n",
    "    times = [t_broadcast_cpu, t_broadcast_gpu, t_cuda_kernel, t_cublas]\n",
    "\n",
    "    flops = saxpy_flops.(times; len)\n",
    "    bandwidths = saxpy_bandwidth.(times; dtype, len)\n",
    "\n",
    "    labels = [\"Broadcast (CPU)\", \"Broadcast (GPU)\", \"CUDA kernel\", \"CUBLAS\"]\n",
    "    data = hcat(labels, 1e3 .* times, flops, bandwidths)\n",
    "    pretty_table(data; header=([\"Variant\", \"Runtime\", \"FLOPS\", \"Bandwidth\"], [\"\", \"ms\", \"GFLOP/s\", \"GB/s\"]))\n",
    "    println(\"Theoretical Memory Bandwidth: 1555 GB/s\")\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────┬─────────┬─────────┬───────────┐\n",
      "│\u001b[1m         Variant \u001b[0m│\u001b[1m Runtime \u001b[0m│\u001b[1m   FLOPS \u001b[0m│\u001b[1m Bandwidth \u001b[0m│\n",
      "│\u001b[90m                 \u001b[0m│\u001b[90m      ms \u001b[0m│\u001b[90m GFLOP/s \u001b[0m│\u001b[90m      GB/s \u001b[0m│\n",
      "├─────────────────┼─────────┼─────────┼───────────┤\n",
      "│ Broadcast (CPU) │ 202.666 │ 5.05265 │   30.3159 │\n",
      "│ Broadcast (GPU) │ 5.09245 │ 201.082 │   1206.49 │\n",
      "│     CUDA kernel │ 4.68801 │  218.43 │   1310.58 │\n",
      "│          CUBLAS │ 4.67729 │  218.93 │   1313.58 │\n",
      "└─────────────────┴─────────┴─────────┴───────────┘\n",
      "Theoretical Memory Bandwidth: 1555 GB/s\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/a100_saxpy_results.png\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "* [KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl): Writing hardware agnostic computational kernels.\n",
    "* [Tullio.jl](https://github.com/mcabbott/Tullio.jl): Also supports NVIDIA GPUs and can produce more efficient kernels than simple broadcasting.\n",
    "\n",
    "More on GPU computing in Julia? See e.g. https://www.youtube.com/watch?v=Hz9IMJuW5hU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
