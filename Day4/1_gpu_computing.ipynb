{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing with Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dgx-01\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gethostname()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topology of a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/gpu_topology.png\" width=1300px>\n",
    "\n",
    "**Source:** [Sivalingam, Karthee. \"GPU Acceleration of a Theoretical Particle Physics Application.\" Master's Thesis, The University of Edinburgh (2010).](https://static.epcc.ed.ac.uk/dissertations/hpc-msc/2009-2010/Karthee%20Sivalingam.pdf)\n",
    "\n",
    "* **SM** = Streaming Multiprocessor\n",
    "* **SP** = Streaming Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA A100 SXM4\n",
    "\n",
    "<img src=\"../imgs/a100_front.png\" width=800px>\n",
    "\n",
    "<img src=\"../imgs/a100_SM.png\" width=400px>\n",
    "\n",
    "**Source:** [NVIDIA whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\n",
    "\n",
    "| Kind                       | Count            |\n",
    "|----------------------------|------------------|\n",
    "| **SMs**                    | 108              |\n",
    "| **CUDA cores** / FP32 ALUs | 6912 (64 per SM) |\n",
    "| **Tensor cores**           | 432 (4 per SM)   |\n",
    "\n",
    "* **ALU** = Arithmetic Logical Unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GPUInspector\n",
    "# gpuinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JuliaGPU\n",
    "\n",
    "Website: https://juliagpu.org/\n",
    "\n",
    "GitHub Org: https://github.com/JuliaGPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(We'll focus on Nvidia GPUs but there is [support for other GPUs](https://juliagpu.org/) as well.)\n",
    "\n",
    "The interface to NVIDIA GPU computing in Julia is [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl).\n",
    "\n",
    "\n",
    "\n",
    "It provides:\n",
    "\n",
    "* **High-level abstraction `CuArray`**\n",
    "* **Tools for writing custom CUDA kernels**\n",
    "* **Wrappers to proprietary NVIDIA libraries (e.g. CUBLAS, CUFFT, CUSPARSE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA toolkit 11.7, artifact installation\n",
      "NVIDIA driver 510.47.3, for CUDA 11.6\n",
      "CUDA driver 11.7\n",
      "\n",
      "Libraries: \n",
      "- CUBLAS: 11.10.1\n",
      "- CURAND: 10.2.10\n",
      "- CUFFT: 10.7.2\n",
      "- CUSOLVER: 11.3.5\n",
      "- CUSPARSE: 11.7.3\n",
      "- CUPTI: 17.0.0\n",
      "- NVML: 11.0.0+510.47.3\n",
      "- CUDNN: 8.30.2 (for CUDA 11.5.0)\n",
      "- CUTENSOR: 1.4.0 (for CUDA 11.5.0)\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.8.0\n",
      "- LLVM: 13.0.1\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0, 7.1, 7.2\n",
      "- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80, sm_86\n",
      "\n",
      "8 devices:\n",
      "  0: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  1: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  2: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  3: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  4: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  5: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  6: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n",
      "  7: NVIDIA A100-SXM4-40GB (sm_80, 39.406 GiB / 40.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo() # automatically downloads CUDA framework if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level abstraction: `CuArray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `CuArray` is a CPU handle to GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_gpu = CuArray{Float32}(undef, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.5646155\n",
       " 0.5066599\n",
       " 0.9901406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.rand(3) # Note: defaults to Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.zeros(3) # Note: defaults to Float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can readily move data to the GPU by converting to `CuArray`.\n",
    "\n",
    " <img src=\"../imgs/cpu_gpu_transfer.svg\" width=180px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_cpu = [1,2,3]\n",
    "x_gpu = CuArray(x_cpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(or by using `copyto!` to move it into already allocated memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CuArray`s are managed by Julia's **garbage collector**. In principle, if they are unreachable, they get cleaned up automatically.\n",
    "\n",
    "However, by default CUDA.jl uses a **memory pool** to speed up allocations. So it might appear as if the objects have not been free'd. (You can disable the pool with `JULIA_CUDA_MEMORY_POOL=none`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 1.13% (455.938 MiB/39.409 GiB)\n",
      "Memory pool usage: 36 bytes (32.000 MiB reserved)"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"38.147 MiB\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Base.format_bytes(sizeof(x_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 1.21% (487.938 MiB/39.409 GiB)\n",
      "Memory pool usage: 38.147 MiB (64.000 MiB reserved)"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = nothing; GC.gc(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 1.21% (487.938 MiB/39.409 GiB)\n",
      "Memory pool usage: 38.147 MiB (64.000 MiB reserved)"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `CUDA.unsafe_free!(x_gpu)` and `CUDA.reclaim()` to more aggressively suggest the freeing of the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA.unsafe_free!(x_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA.reclaim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CuArray <: AbstractArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we should be able to do all kind of operations with it, that we'd also do with regular `Array`s. (**duck typing**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 0.69478    0.0289907  0.0183466  …  0.339742   0.0545384  0.965939\n",
       " 0.817242   0.0547828  0.302184      0.717154   0.561394   0.473867\n",
       " 0.622881   0.495846   0.0230539     0.0381533  0.583806   0.477023\n",
       " 0.447436   0.803525   0.71222       0.395164   0.352538   0.309192\n",
       " 0.964878   0.442542   0.370591      0.0926875  0.624995   0.847689\n",
       " 0.665145   0.770985   0.0382734  …  0.533212   0.416791   0.626831\n",
       " 0.483884   0.279765   0.484469      0.660593   0.520294   0.422101\n",
       " 0.166863   0.525936   0.257448      0.308513   0.497302   0.289458\n",
       " 0.647593   0.616969   0.548403      0.304596   0.581397   0.283971\n",
       " 0.0499254  0.882947   0.675325      0.72163    0.532929   0.825365\n",
       " ⋮                                ⋱                        \n",
       " 0.846527   0.383507   0.746302      0.18618    0.990434   0.119101\n",
       " 0.956102   0.719037   0.424064      0.166237   0.179259   0.714286\n",
       " 0.959444   0.0528611  0.528541      0.674956   0.61374    0.762448\n",
       " 0.393836   0.865451   0.185435      0.310034   0.261209   0.38767\n",
       " 0.0605131  0.17776    0.732418   …  0.0396937  0.699901   0.570296\n",
       " 0.841339   0.647407   0.93506       0.900638   0.53533    0.0499651\n",
       " 0.851715   0.603457   0.0783542     0.261396   0.553388   0.281218\n",
       " 0.456991   0.36058    0.132912      0.317694   0.44901    0.739751\n",
       " 0.371026   0.59952    0.398978      0.466631   0.019047   0.795475"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 1000\n",
    "A_gpu = CUDA.rand(N,N)\n",
    "B_gpu = CUDA.rand(N,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 263.231  250.303  249.849  246.12   …  251.736  264.457  255.108  261.233\n",
       " 266.702  262.424  256.225  257.277     262.914  270.869  258.223  262.323\n",
       " 258.117  253.468  243.063  248.039     252.824  261.536  256.234  256.848\n",
       " 255.43   250.546  248.836  246.455     252.775  262.942  255.662  256.531\n",
       " 255.411  247.417  243.252  244.947     244.026  254.167  244.498  248.254\n",
       " 253.226  242.999  246.033  241.936  …  247.346  254.422  242.234  251.317\n",
       " 257.207  253.969  246.566  241.779     248.356  258.27   251.288  252.969\n",
       " 257.595  252.352  248.305  246.535     250.534  257.406  255.271  255.107\n",
       " 256.171  253.301  251.302  247.941     247.637  257.632  256.604  253.424\n",
       " 252.13   247.081  244.887  240.462     246.285  259.547  252.714  250.765\n",
       "   ⋮                                 ⋱                             \n",
       " 264.99   264.5    263.187  253.443     260.07   269.754  261.162  268.605\n",
       " 256.06   253.018  247.612  244.237     249.991  253.201  251.738  248.555\n",
       " 260.134  252.804  255.854  248.905     253.893  264.385  255.57   261.341\n",
       " 261.22   259.733  261.698  260.185     263.705  272.575  265.417  262.837\n",
       " 263.812  250.639  252.764  247.516  …  250.984  262.077  255.592  260.432\n",
       " 254.703  242.958  240.435  246.608     249.651  259.329  254.746  252.739\n",
       " 258.933  252.476  251.225  245.447     251.89   262.227  250.13   258.184\n",
       " 254.639  247.877  243.755  242.523     247.033  256.245  247.632  255.098\n",
       " 251.536  243.944  242.999  241.103     247.529  253.442  242.877  253.598"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_gpu * B_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.632 ms (2 allocations: 3.81 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  17.894 μs (29 allocations: 592 bytes)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime A_cpu * B_cpu setup=(A_cpu = rand(Float32, N,N); B_cpu = rand(Float32, N,N););\n",
    "@btime A_gpu * B_gpu setup=(A_gpu = CUDA.rand(N,N); B_gpu = CUDA.rand(N,N););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the timescales change: **milliseconds -> microseconds**!\n",
    "\n",
    "(`*` for `CuArray`s uses a cuBLAS kernel under the hood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Broadcasting, `map`, `reduce`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 1.40741   0.19631   0.316908   0.438691  …  0.479184  0.841024  1.94615\n",
       " 0.986012  0.747058  0.903933   0.811005     0.973116  1.40697   1.30586\n",
       " 0.858665  1.24315   0.718586   0.793501     0.57865   0.81746   0.797016\n",
       " 1.29973   1.03181   0.997126   1.21855      1.18421   0.629287  0.696938\n",
       " 1.90947   0.891688  1.14663    0.593971     0.471222  0.696889  1.30617\n",
       " 1.48993   1.64138   0.57461    0.207497  …  1.03745   0.440282  1.62009\n",
       " 0.637022  0.685722  1.41646    0.950116     0.926303  0.94046   1.11532\n",
       " 0.941927  0.530214  0.517546   0.317172     1.03441   1.3146    0.966244\n",
       " 0.693739  0.851431  1.35988    0.840186     1.08617   0.685314  0.310763\n",
       " 0.558172  1.37046   1.22602    0.76166      0.837085  1.21962   1.3164\n",
       " ⋮                                        ⋱                      \n",
       " 1.29783   0.418178  0.803072   0.954879     0.999511  1.59043   0.516183\n",
       " 1.49435   0.725889  1.06637    0.385113     1.02486   0.717913  0.791807\n",
       " 1.62787   0.621805  0.654338   1.49778      1.39612   0.643062  0.863411\n",
       " 1.18457   1.6646    0.750944   0.595655     0.380039  1.00829   0.409995\n",
       " 1.00728   1.11553   1.71797    0.540847  …  0.874144  1.50042   1.4663\n",
       " 1.37228   0.766034  1.57999    0.479138     1.62149   1.27788   0.719457\n",
       " 1.75133   0.676     0.0809478  0.920085     0.786707  0.840182  1.1168\n",
       " 0.960296  0.580157  0.514426   1.13313      1.29286   0.641905  1.51913\n",
       " 0.647548  0.76133   1.22614    0.300085     0.492885  0.621178  1.72555"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A_gpu .+ B_gpu # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 0.995272  0.169813  0.299124   0.381416  …  0.367245  0.788375  1.37617\n",
       " 0.834487  0.69444   0.673362   0.605327     0.761463  1.01497   0.95748\n",
       " 0.666014  0.896846  0.695914   0.730457     0.541842  0.628827  0.57441\n",
       " 0.962606  0.835325  0.767091   0.862003     0.88247   0.448188  0.49593\n",
       " 1.35027   0.630536  0.859987   0.592642     0.389717  0.629117  0.963734\n",
       " 1.05957   1.16276   0.537701   0.148388  …  0.733872  0.417452  1.17451\n",
       " 0.507538  0.493021  1.05039    0.74004      0.712029  0.668764  0.811618\n",
       " 0.792822  0.525953  0.365965   0.22832      0.788737  0.956706  0.736088\n",
       " 0.649235  0.660018  0.979412   0.693964     0.838834  0.590611  0.285232\n",
       " 0.510693  1.00859   0.871396   0.596788     0.730807  0.86923   0.960385\n",
       " ⋮                                        ⋱                      \n",
       " 0.959315  0.385071  0.748458   0.705959     0.834368  1.15799   0.41456\n",
       " 1.0972    0.719069  0.769663   0.289522     0.874567  0.567699  0.71848\n",
       " 1.16933   0.571395  0.543306   1.1125       0.987749  0.61444   0.769103\n",
       " 0.883381  1.17798   0.595135   0.473889     0.317839  0.791429  0.388312\n",
       " 0.948698  0.954465  1.2279     0.421695  …  0.835394  1.06334   1.0621\n",
       " 0.99486   0.658185  1.1359     0.389648     1.15359   0.915397  0.671354\n",
       " 1.23884   0.607801  0.0783971  0.652921     0.586753  0.623289  0.881637\n",
       " 0.679821  0.422175  0.404003   0.968723     1.02561   0.48869   1.07455\n",
       " 0.462736  0.620972  0.91836    0.224959     0.467369  0.602432  1.22386"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqrt.(A_gpu.^2 + B_gpu.^2) # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459606.56f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapreduce(sin, +, A_gpu) # runs on the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Counter-example:\" Scalar indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore are only permitted from the REPL for prototyping purposes.\n",
      "If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base ./error.jl:35\n",
      " [2] assertscalar(op::String)\n",
      "   @ GPUArraysCore /scratch/pc2-mitarbeiter/bauerc/.julia/packages/GPUArraysCore/lojQM/src/GPUArraysCore.jl:87\n",
      " [3] getindex(xs::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, I::Int64)\n",
      "   @ GPUArrays /scratch/pc2-mitarbeiter/bauerc/.julia/packages/GPUArrays/fqD8z/src/host/indexing.jl:9\n",
      " [4] top-level scope\n",
      "   @ /scratch/pc2-mitarbeiter/bauerc/teaching/JuliaHLRS22/Day4/1_gpu_computing.ipynb:1"
     ]
    }
   ],
   "source": [
    "A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7126331f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.@allowscalar A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu_broadcasting! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function gpu_not_actually!(C, A, B)\n",
    "    CUDA.@sync CUDA.@allowscalar for i in eachindex(A,B)\n",
    "        C[i] = A[i] * B[i] # multiplication will happen on CPU!\n",
    "    end\n",
    "end\n",
    "\n",
    "function gpu_broadcasting!(C, A, B)\n",
    "    CUDA.@sync C .= A .* B\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.906 ms"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (601 allocations: 135.95 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  15.750 μs (24 allocations: 1.83 KiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "N = 10\n",
    "@btime gpu_not_actually!(C, A, B) setup=(A = CUDA.rand(10,10); B = CUDA.rand(10,10); C = CUDA.rand(10,10););\n",
    "@btime gpu_broadcasting!(C, A, B) setup=(A = CUDA.rand(10,10); B = CUDA.rand(10,10); C = CUDA.rand(10,10););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FLoops: CUDA executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu_floops! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using FLoops, FoldsCUDA\n",
    "\n",
    "function gpu_floops!(C, A, B)\n",
    "    CUDA.@sync @floop CUDAEx() for i in eachindex(A,B,C)\n",
    "        C[i] = A[i] * B[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  52.169 μs (237 allocations: 19.67 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime gpu_floops!(C, A, B) setup=(A = CUDA.rand(10,10); B = CUDA.rand(10,10); C = CUDA.rand(10,10););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel programming: Writing CUDA kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CUDA kernel is a function that will be executed by all GPU *threads* separately. Based on the index of a thread we can make them operate on different pieces of given data (Single Program Multiple Data (SPMD) programming model similar to MPI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuda_kernel! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = CUDA.zeros(1024)\n",
    "\n",
    "function cuda_kernel!(x)\n",
    "    i = threadIdx().x\n",
    "    x[i] += 1\n",
    "    return nothing # CUDA kernels should never return anything\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(cuda_kernel!), Tuple{CuDeviceVector{Float32, 1}}}(cuda_kernel!, CuFunction(Ptr{Nothing} @0x000000003e1674f0, CuModule(Ptr{Nothing} @0x000000003ec4e3e0, CuContext(0x0000000002caaa60, instance f2e7009325be14e3))), CUDA.KernelState(Ptr{Nothing} @0x00007f45aa400000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ⋮\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel programming can quickly become (much) more difficult though because\n",
    "* you need to respect **hardware limitations** of the GPU\n",
    "* **not all operations can readily be expressed as scalar kernels** (e.g. reductions)\n",
    "* since kernels execute on the GPU, the Julia runtime isn't available and kernel code has limitations (**you can't just write arbitrary Julia code in kernels**)\n",
    "  * no GC / no allocations\n",
    "  * must be fully type inferred\n",
    "  * no `try ... catch ... end`\n",
    "  * no strings\n",
    "  * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example for a hardware limitation: **A100 supports a maximal number of 1024 threads.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "CuError",
     "evalue": "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
     "output_type": "error",
     "traceback": [
      "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] throw_api_error(res::CUDA.cudaError_enum)\n",
      "    @ CUDA /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/error.jl:89\n",
      "  [2] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/error.jl:97 [inlined]\n",
      "  [3] cuLaunchKernel\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/utils/call.jl:26 [inlined]\n",
      "  [4] (::CUDA.var\"#39#40\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3})(kernelParams::Vector{Ptr{Nothing}})\n",
      "    @ CUDA /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:69\n",
      "  [5] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:33 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ ./none:0 [inlined]\n",
      "  [7] pack_arguments(::CUDA.var\"#39#40\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3}, ::CUDA.KernelState, ::CuDeviceVector{Float32, 1})\n",
      "    @ CUDA ./none:0\n",
      "  [8] #launch#38\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:62 [inlined]\n",
      "  [9] #44\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:136 [inlined]\n",
      " [10] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:95 [inlined]\n",
      " [11] macro expansion\n",
      "    @ ./none:0 [inlined]\n",
      " [12] convert_arguments\n",
      "    @ ./none:0 [inlined]\n",
      " [13] #cudacall#43\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/lib/cudadrv/execution.jl:135 [inlined]\n",
      " [14] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:204 [inlined]\n",
      " [15] macro expansion\n",
      "    @ ./none:0 [inlined]\n",
      " [16] #call#207\n",
      "    @ ./none:0 [inlined]\n",
      " [17] (::CUDA.HostKernel{typeof(cuda_kernel!), Tuple{CuDeviceVector{Float32, 1}}})(args::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}; threads::Int64, blocks::Int64, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})\n",
      "    @ CUDA /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:484\n",
      " [18] macro expansion\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/src/compiler/execution.jl:104 [inlined]\n",
      " [19] top-level scope\n",
      "    @ /scratch/pc2-mitarbeiter/bauerc/.julia/packages/CUDA/DfvRa/src/utilities.jl:25"
     ]
    }
   ],
   "source": [
    "x = CUDA.zeros(1025)\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But what if we want to go larger?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA programming model\n",
    "\n",
    "<img src=\"../imgs/cuda_blocks_threads.png\" width=700px>\n",
    "\n",
    "(Note: in Julia indices start at 1)\n",
    "\n",
    "**Source:** [Sivalingam, Karthee. \"GPU Acceleration of a Theoretical Particle Physics Application.\" Master's Thesis, The University of Edinburgh (2010).](https://static.epcc.ed.ac.uk/dissertations/hpc-msc/2009-2010/Karthee%20Sivalingam.pdf)\n",
    "\n",
    "* **Threads** → CUDA cores\n",
    "* **Blocks** of threads → SMs\n",
    "* **Grid** of blocks → entire GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuda_kernel_blocks! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cuda_kernel_blocks!(x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(x)\n",
    "        @inbounds x[i] += 1\n",
    "    end\n",
    "    return nothing # CUDA kernels should never return anything\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(cuda_kernel_blocks!), Tuple{CuDeviceVector{Float32, 1}}}(cuda_kernel_blocks!, CuFunction(Ptr{Nothing} @0x000000004112e5f0, CuModule(Ptr{Nothing} @0x000000003e861280, CuContext(0x0000000002caaa60, instance f2e7009325be14e3))), CUDA.KernelState(Ptr{Nothing} @0x00007f45aa400000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = CUDA.zeros(1025)\n",
    "CUDA.@sync @cuda threads=1024 blocks=2 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ⋮\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our custom CUDA kernel compare to broadcasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  13.234 μs (5 allocations: 304 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16.160 μs (30 allocations: 1.58 KiB)\n"
     ]
    }
   ],
   "source": [
    "function add_one_kernel(x)\n",
    "    CUDA.@sync @cuda threads=1024 blocks=1 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "function add_one_broadcasting(x)\n",
    "    CUDA.@sync x .+ 1\n",
    "end\n",
    "\n",
    "@btime add_one_kernel(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_broadcasting(x) setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying kernel launches: [Occupancy API](https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/)\n",
    "\n",
    "Hardcoding limits (1024 above) is rarely a good idea. Also, in reality, the actual maximal number of threads can depend on kernel details, like how many resources the kernel is using.\n",
    "\n",
    "**The occupancy API is an automatic tool that can be used to obtain good launch parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(cuda_kernel_blocks!), Tuple{CuDeviceVector{Float32, 1}}}(cuda_kernel_blocks!, CuFunction(Ptr{Nothing} @0x000000004112e5f0, CuModule(Ptr{Nothing} @0x000000003e861280, CuContext(0x0000000002caaa60, instance f2e7009325be14e3))), CUDA.KernelState(Ptr{Nothing} @0x00007f45aa400000))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel = @cuda launch=false cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.maxthreads(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(blocks = 216, threads = 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = CUDA.launch_configuration(kernel.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the number `blocks` indicates how many blocks we would need to fully occupy the GPU. For a given input `x`, we might need fewer or more blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threads = min(length(x), config.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blocks = cld(length(x), threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the kernel with the dynamic launch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel(x; threads, blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Three ways to SAXPY on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAXPY** = **S**ingle precision **A** times **X** **P**lus **Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_broadcast_cpu!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Computes the SAXPY on the CPU using broadcasting\"\n",
    "function saxpy_broadcast_cpu!(a, x, y)\n",
    "    y .= a .* x .+ y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_broadcast_gpu!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Computes the SAXPY on the GPU using broadcasting\"\n",
    "function saxpy_broadcast_gpu!(a, x, y)\n",
    "    CUDA.@sync y .= a .* x .+ y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_cuda_kernel!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"CUDA kernel for computing the SAXPY on the GPU\"\n",
    "function _saxpy_kernel!(a, x, y)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(y)\n",
    "        @inbounds y[i] = a * x[i] + y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\"Computes the SAXPY on the GPU using the custom CUDA kernel `_saxpy_kernel!`\"\n",
    "function saxpy_cuda_kernel!(a, x, y; nthreads, nblocks)\n",
    "    CUDA.@sync @cuda(\n",
    "        threads = nthreads,\n",
    "        blocks = nblocks,\n",
    "        _saxpy_kernel!(a, x, y)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_cublas! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function saxpy_cublas!(a, x, y)\n",
    "    CUDA.@sync CUBLAS.axpy!(length(x), a, x, y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using PrettyTables\n",
    "\n",
    "\"Computes the GFLOP/s from the vector length `len` and the measured runtime `t`.\"\n",
    "saxpy_flops(t; len) = 2.0 * len * 1e-9 / t # GFLOP/s\n",
    "\n",
    "\"Computes the GB/s from the vector length `len`, the vector element type `dtype`, and the measured runtime `t`.\"\n",
    "saxpy_bandwidth(t; dtype, len) = 3.0 * sizeof(dtype) * len * 1e-9 / t # GB/s\n",
    "\n",
    "function main()\n",
    "    if !contains(lowercase(name(device())), \"a100\")\n",
    "        @warn(\"This script was tuned for a NVIDIA A100 GPU. Your GPU: $(name(device())).\")\n",
    "    end\n",
    "    dtype = Float32\n",
    "    nthreads = 1024 # CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK\n",
    "    nblocks = 500_000\n",
    "    len = nthreads * nblocks # vector length\n",
    "    a = convert(dtype, 3.1415)\n",
    "    x = ones(dtype, len)\n",
    "    y = ones(dtype, len)\n",
    "    xgpu = CUDA.ones(dtype, len)\n",
    "    ygpu = CUDA.ones(dtype, len)\n",
    "\n",
    "    t_broadcast_cpu = @belapsed saxpy_broadcast_cpu!($a, $x, $y) samples = 10 evals = 2\n",
    "    t_broadcast_gpu = @belapsed saxpy_broadcast_gpu!($a, $xgpu, $ygpu) samples = 10 evals = 2\n",
    "    t_cuda_kernel = @belapsed saxpy_cuda_kernel!($a, $xgpu, $ygpu; nthreads=$nthreads, nblocks=$nblocks) samples = 10 evals = 2\n",
    "    t_cublas = @belapsed saxpy_cublas!($a, $xgpu, $ygpu) samples = 10 evals = 2\n",
    "    times = [t_broadcast_cpu, t_broadcast_gpu, t_cuda_kernel, t_cublas]\n",
    "\n",
    "    flops = saxpy_flops.(times; len)\n",
    "    bandwidths = saxpy_bandwidth.(times; dtype, len)\n",
    "\n",
    "    labels = [\"Broadcast (CPU)\", \"Broadcast (GPU)\", \"CUDA kernel\", \"CUBLAS\"]\n",
    "    data = hcat(labels, 1e3 .* times, flops, bandwidths)\n",
    "    pretty_table(data; header=([\"Variant\", \"Runtime\", \"FLOPS\", \"Bandwidth\"], [\"\", \"ms\", \"GFLOP/s\", \"GB/s\"]))\n",
    "    println(\"Theoretical Memory Bandwidth: 1555 GB/s\")\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────┬─────────┬─────────┬───────────┐\n",
      "│\u001b[1m         Variant \u001b[0m│\u001b[1m Runtime \u001b[0m│\u001b[1m   FLOPS \u001b[0m│\u001b[1m Bandwidth \u001b[0m│\n",
      "│\u001b[90m                 \u001b[0m│\u001b[90m      ms \u001b[0m│\u001b[90m GFLOP/s \u001b[0m│\u001b[90m      GB/s \u001b[0m│\n",
      "├─────────────────┼─────────┼─────────┼───────────┤\n",
      "│ Broadcast (CPU) │ 202.666 │ 5.05265 │   30.3159 │\n",
      "│ Broadcast (GPU) │ 5.09245 │ 201.082 │   1206.49 │\n",
      "│     CUDA kernel │ 4.68801 │  218.43 │   1310.58 │\n",
      "│          CUBLAS │ 4.67729 │  218.93 │   1313.58 │\n",
      "└─────────────────┴─────────┴─────────┴───────────┘\n",
      "Theoretical Memory Bandwidth: 1555 GB/s\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/a100_saxpy_results.png\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "* [KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl): Writing hardware agnostic computational kernels.\n",
    "* [Tullio.jl](https://github.com/mcabbott/Tullio.jl): Also supports NVIDIA GPUs and can produce more efficient kernels than simple broadcasting.\n",
    "\n",
    "More on GPU computing in Julia? See e.g. https://www.youtube.com/watch?v=Hz9IMJuW5hU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
