{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing: Message Passing Interface (MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Distributed` vs MPI\n",
    "\n",
    "**`Distributed`**\n",
    "* can be **convenient**, in particular for **\"ad-hoc\" distributed computing** (e.g. data processing)\n",
    "* **master-worker model** often naturally aligns with the structure of scientific computations\n",
    "* can be used **interactively** in a REPL / in Jupyter etc.\n",
    "* no external dependencies, **built-in** library\n",
    "* higher overhead than MPI and doesn't scale as well (doesn't utilizie Infiniband -> slower communication)\n",
    "\n",
    "**MPI**\n",
    "* **de-facto industry standard** for massively parallel computing, e.g. large scale distributed computing\n",
    "* **known to scale well** up to thousands of compute nodes\n",
    "* does utilize **Infiniband**\n",
    "* Programming model can be more challenging\n",
    "* No (or poor) interactivity (see [MPIClusterManager.jl](https://github.com/JuliaParallel/MPIClusterManagers.jl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI and MPI.jl\n",
    "\n",
    "* **[MPI](https://www.mpi-forum.org/)**: A standard with several specific implementations (e.g. OpenMPI, IntelMPI, MPICH)\n",
    "* **[MPI.jl](https://github.com/JuliaParallel/MPI.jl)**: Julia package and interface to MPI implementations\n",
    "\n",
    "### How to get an MPI implementation?\n",
    "\n",
    "* Will be automatically downloaded when installing MPI.jl (`] add MPI`).\n",
    "* Alternative: Install manually (e.g. from https://www.open-mpi.org/) and point MPI.jl to the manual installation via `ENV[\"JULIA_MPI_BINARY\"]=\"system\"`.\n",
    "* On clusters (e.g. Hawk): Often provided as a module. If it doesn't work out of the box then use `ENV[\"JULIA_MPI_BINARY\"]=\"system\"` [and partners](https://juliaparallel.org/MPI.jl/stable/configuration/#environment_variables).\n",
    "\n",
    "### Programming model and execution\n",
    "\n",
    "MPI programming model:\n",
    "* **conceptually, all processes execute the same program**.\n",
    "* different behavior for processes must be implementend with conditionals (e.g. using rank information)\n",
    "* individual processes flow at there own pace (they can get out of sync).\n",
    "* selecting the concrete number of processes is deferred to \"runtime\".\n",
    "\n",
    "#### Example: Hello World\n",
    "\n",
    "```julia\n",
    "# file: mpi_hello.jl\n",
    "using MPI\n",
    "MPI.Init()\n",
    "comm = MPI.COMM_WORLD\n",
    "print(\"Hello world, I am rank $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\\n\")\n",
    "MPI.Finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental MPI functions\n",
    "\n",
    "`MPI.Init()` and `MPI.Finalize()`: Always at the top or bottom of your code, respectively.\n",
    "\n",
    "`MPI.COMM_WORLD`: default communicator (group of MPI processes) which includes all processes created when launching the program\n",
    "\n",
    "`MPI.Comm_rank(comm)`: rank of the process calling this function\n",
    "\n",
    "`MPI.Comm_size(comm)`: total number of processes in the given communicator\n",
    "\n",
    "**Naming convention in MPI.jl**\n",
    "* If possible, `MPI_*` in C -> `MPI.*` in Julia\n",
    "* Examples:\n",
    "  * `MPI_COMM_WORLD` -> `MPI.COMM_WORLD`\n",
    "  * `MPI_Comm_size` -> `MPI.Comm_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running an MPI code\n",
    "\n",
    "MPI implementations provide `mpirun` and/or `mpiexec` to run MPI applications.\n",
    "\n",
    "`mpirun -n <number_of_processes) julia --project mycode.jl`\n",
    "\n",
    "(If you want to use the MPI that automatically ships with MPI.jl you should use the [`mpiexecjl` wrapper](https://juliaparallel.org/MPI.jl/stable/configuration/#Julia-wrapper-for-mpiexec).)\n",
    "\n",
    "<img src=\"../imgs/julia_mpi_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic communication\n",
    "\n",
    "##### Two-sided, blocking\n",
    "\n",
    "* `MPI.Send(data, destination, tag, communicator)` and `MPI.Recv(data, origin, tag, communicator)`\n",
    "  * `data`: For example an array (buffer)\n",
    "  * `destination` / `origin`: Rank of the target process\n",
    "  * `tag`: \"optional\" integer (just set it to zero)\n",
    "  * `communicator`\n",
    "\n",
    "* `MPI.Recv(data, origin, tag, communicator)`\n",
    "  * `data`: For example an array (buffer)\n",
    "  * `destination` / `origin`: Rank of source process\n",
    "  * `tag`: \"optional\" integer (just set it to zero)\n",
    "  * `communicator`\n",
    "\n",
    "Blocking, so be aware of deadlocks! (There are `MPI.Sendrecv!` and the non-blocking variants `MPI.Isend` and `MPI.Irecv!`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collectives\n",
    "\n",
    "* **Synchronization**:\n",
    "  * `MPI.Barrier(comm)`\n",
    "* **Data movement**:\n",
    "  * one-to-many and many-to-many (e.g. broadcast, scatter, gather, all to all)\n",
    "  * `result = MPI.Bcast!(data, root, communicator)`\n",
    "    * `data`: For example an array (buffer)\n",
    "    * `root`: root rank (should hold the data)\n",
    "    * `communicator`\n",
    "* **Reduction**:\n",
    "  * `result = MPI.Reduce(local_data, op, root, communicator)`\n",
    "    * `local_data`: For example an array (buffer)\n",
    "    * `op`: reducer function, e.g. `+` \n",
    "    * `root`: root rank (will eventually hold the reduction result)\n",
    "    * `communicator`\n",
    "\n",
    "<img src=\"../imgs/mpi_reduction.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conveniences of MPI.jl\n",
    "\n",
    "* Julia MPI functions can have **less function arguments** than C counterparts if some of them are deducible\n",
    "* MPI functions can often handle data of **built-in and custom Julia types** (i.e. custom `struct`s)\n",
    "  * `MPI.Types.create_*` constructor functions (`create_vector`, `create_subarray`, `create_struct`, etc.) get automatically called under the hood.\n",
    "* MPI Functions can often handle **built-in and custom Julia functions**, e.g. as a reducer function in `MPI.Reduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level tools\n",
    "\n",
    "* [PartitionedArrays.jl](https://github.com/fverdugo/PartitionedArrays.jl): Data-oriented parallel implementation of partitioned vectors and sparse matrices needed in FD, FV, and FE simulations.\n",
    "* [Elemental.jl](https://github.com/JuliaParallel/Elemental.jl): A package for dense and sparse distributed linear algebra and optimization.\n",
    "* [PETSc.jl](): Suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. ([original website](https://petsc.org/release/))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
