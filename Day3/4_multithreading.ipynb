{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are threads?\n",
    "Threads are execution units within a process that can run simultaneously.\n",
    "\n",
    "<img src=\"../imgs/processes_threads.png\" width=400px>\n",
    "\n",
    "While processes are entirely separate, threads run in a **shared memory** space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Julia with multiple threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia starts with a single *user thread*. We must tell it explicitly to start multiple user threads. There are two ways to do this:\n",
    "\n",
    "* Environment variable: `JULIA_NUM_THREADS=4`\n",
    "* Command line argument: `julia -t 4` or, equivalently, `julia --threads 4`\n",
    "\n",
    "**Jupyter lab:**\n",
    "\n",
    "The simplest way is to globally set the environment variable `JULIA_NUM_THREADS` (e.g. in the `.bashrc`). But one can also create a specific Jupyter kernel for multithreaded Julia:\n",
    "\n",
    "```julia\n",
    "using IJulia\n",
    "installkernel(\"Julia (4 threads)\", env=Dict(\"JULIA_NUM_THREADS\"=>\"4\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily check how many threads we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User threads vs default threads\n",
    "\n",
    "Technically, the Julia process is also spawning multiple threads already in \"single-threaded\" mode, like\n",
    "* a thread for unix signal listening\n",
    "* multiple OpenBLAS threads for BLAS/LAPACK operations\n",
    "\n",
    "For this reason, we call the threads specified via `--threads` or the environment variable *user threads* or simply *Julia threads*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-based multithreading\n",
    "\n",
    "Conceptually, Julia implements **task-based** multithreading. In this paradigm, a task - e.g. a computational piece of a code - is marked for parallel execution on **any** of the available Julia threads. Julias **dynamic scheduler** will automatically put the task on one of the threads and trigger the execution of the task on said thread.\n",
    "\n",
    "Ideally, **a user should think about tasks and not threads**.\n",
    "\n",
    "**Advantages:**\n",
    "* high-level and convenient\n",
    "* **composability / nestability** (Multithreaded code can call multithreaded code can call multithreaded code ....)\n",
    "\n",
    "**Disadvantages:**\n",
    "* **scheduling overhead**\n",
    "* can get in the way when performance engineering\n",
    "  * scheduler has limited information (e.g. about the system topology)\n",
    "  * low-level profiling (e.g. with LIKWID) currently requires a known task -> thread -> cpu core mapping.\n",
    "\n",
    "(Blog post: [Announcing composable multi-threaded parallelism in Julia](https://julialang.org/blog/2019/07/multithreading/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawning tasks on threads: `Threads.@spawn`\n",
    "`Threads.@spawn` spawns a task on a Julia thread. Specifically, it creates (and immediately returns) a `Task` and schedules it for execution on an available Julia thread.\n",
    "\n",
    "Note the conceptual similarity between `Threads.@spawn` (task -> thread) and `Distributed.@spawn` (task -> process) and also `@async`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to prefix `Threads.` to `@spawn` (and other threading-related functions) let's load everything from `Base.Threads` into global scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@spawn println(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `Threads.@spawn` returns the task right away - it is **non-blocking** - the result might only be fetchable after some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = @spawn begin\n",
    "    sleep(3);\n",
    "    \"result\"\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use (some of) the control flow tools that we've already covered, like `@sync`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync t = @spawn begin\n",
    "    sleep(3);\n",
    "    \"result\"\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:2*nthreads()\n",
    "    @spawn println(\"Hi, I'm \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Recursive Fibonacci series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F(n) = F(n-1) + F(n-2), \\qquad F(1) = F(2) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can nest `@spawn` calls freely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fib(n)\n",
    "    n < 2 && return n\n",
    "    t = @spawn fib(n-2)\n",
    "    return fib(n-1) + fetch(t)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fib.(1:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: Algorithmically, this is a highly inefficient implementation of the Fibonacci series, of course!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `tmap` (like `pmap`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(fn, itr) = map(fetch, map(i -> Threads.@spawn(fn(i)), itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [rand(200,200) for i in 1:10];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(svdvals, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(i -> println(i, \" ($(threadid()))\"), 1:10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that this implementation creates temporary allocations and thus isn't particularly efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime tmap($svdvals, $M);\n",
    "@btime map($svdvals, $M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks on `@spawn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Task migration**: Not only does the scheduler dynamically assign tasks to Julia threads, but it is also free to move tasks between threads. Hence, `threadid()` isn't necessarily constant over time and should be used with care!\n",
    "* **Spawning tasks on specific threads**: Julia doesn't have a built-in tool for this (as of now). However, some packages like [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl) export `@tspawnat <threadid> ...` which allows to spawn *sticky* tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ThreadPinning\n",
    "\n",
    "@tspawnat 3 println(\"running on thread \", threadid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading for-loops: `@threads`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher level interface to multithreading. (Compare `Distributed.@spawnat` vs `@distributed`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads for i in 1:2*nthreads()\n",
    "    println(\"Hi, I'm \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "function square!(x)\n",
    "    for i in eachindex(x)\n",
    "        x[i] = x[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "function square_threads!(x)\n",
    "    @threads for i in eachindex(x)\n",
    "        x[i] = x[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "x = rand(1_000_000)\n",
    "@btime square!($x);\n",
    "@btime square_threads!($x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduling options\n",
    "\n",
    "Syntax: `@threads [schedule] for ...`\n",
    "\n",
    "  * `:dynamic` (default)\n",
    "    * creates O(`nthreads()`) many tasks each processing a contigious region of the iteration space\n",
    "    * each task essentially spawned with `@spawn`\n",
    "      * -> task migration\n",
    "      * -> composability / nestability\n",
    "    \n",
    "  * `:static`\n",
    "    * evenly splits up the iteration space and creates one task per block\n",
    "    * **statically** maps tasks to threads, specifically: task 1 -> thread 1, task 2 -> thread 2, etc.\n",
    "      * -> no task migration, i.e. **fixed task-thread mapping**\n",
    "      * -> not composable / nestable\n",
    "      * -> only little overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :dynamic for i in 1:2*nthreads()\n",
    "    println(i, \" -> thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :static for i in 1:2*nthreads()\n",
    "    println(i, \" -> thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `@threads :static`, every thread handles precisely two iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :dynamic for i in 1:3\n",
    "    @threads :dynamic for j in 1:3\n",
    "        println(\"$i, $j\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :static for i in 1:3\n",
    "    @threads :static for j in 1:3\n",
    "        println(\"$i, $j\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load-balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_nonuniform_spawn!(a, niter = zeros(Int, nthreads()), load = zeros(Int, nthreads()))\n",
    "    @sync for i in 1:length(a)\n",
    "        Threads.@spawn begin\n",
    "            a[i] = sum(abs2, rand() for j in 1:i)\n",
    "            \n",
    "            # only for bookkeeping\n",
    "            niter[threadid()] += 1\n",
    "            load[threadid()] += i\n",
    "        end\n",
    "    end\n",
    "    return niter, load\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zeros(nthreads()*20)\n",
    "niter, load = compute_nonuniform_spawn!(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "b1 = bar(niter, xlab=\"threadid\", ylab=\"# iterations\", title=\"Number of iterations\", legend=false)\n",
    "b2 = bar(load, xlab=\"threadid\", ylab=\"workload\", title=\"Workload\", legend=false)\n",
    "\n",
    "display(b1)\n",
    "display(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_nonuniform_threads!(a, niter = zeros(Int, nthreads()), load = zeros(Int, nthreads()))\n",
    "    @threads for i in 1:length(a)\n",
    "        a[i] = sum(abs2, rand() for j in 1:i)\n",
    "\n",
    "        # only for bookkeeping\n",
    "        niter[threadid()] += 1\n",
    "        load[threadid()] += i\n",
    "    end\n",
    "    return niter, load\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zeros(nthreads()*20)\n",
    "niter, load = compute_nonuniform_threads!(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = bar(niter, xlab=\"threadid\", ylab=\"# iterations\", title=\"Number of iterations\", legend=false)\n",
    "b2 = bar(load, xlab=\"threadid\", ylab=\"workload\", title=\"Workload\", legend=false)\n",
    "\n",
    "display(b1)\n",
    "display(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(There might be a scheduling option for `@threads` that implements load-balancing in the future.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading: Things to be aware of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race conditions and thread safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_serial(x)\n",
    "    s = zero(eltype(x))\n",
    "    for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_naive(x)\n",
    "    s = zero(eltype(x))\n",
    "    @threads for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = rand(nthreads()*10_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sum(numbers);\n",
    "@show sum_serial(numbers);\n",
    "@show sum_threads_naive(numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrong** result! Even worse, it's **non-deterministic** and different every time! It's also slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_serial($numbers);\n",
    "@btime sum_threads_naive($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason: There is a [race condition](https://en.wikipedia.org/wiki/Race_condition).\n",
    "\n",
    "Note that race conditions aren't specific to reductions. More generally, they can appear when multiple threads are modifying a shared \"global\" state simultaneously.\n",
    "\n",
    "Not all of Julia and its packages in the ecosystem are thread-safe! In general, it is safer to assume that they're not unless proven otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix 1: Divide the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_threads_subsums(x)\n",
    "    blocksize = length(x) ÷ nthreads()\n",
    "    @assert isinteger(blocksize)\n",
    "    idcs = collect(Iterators.partition(1:length(x), blocksize))\n",
    "    \n",
    "    subsums = zeros(eltype(x), nthreads())\n",
    "    @threads for tid in 1:nthreads()\n",
    "        for i in idcs[tid]\n",
    "            @inbounds subsums[tid] += x[i]\n",
    "        end\n",
    "    end\n",
    "    return sum(subsums)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sum(numbers);\n",
    "@show sum_serial(numbers);\n",
    "@show sum_threads_subsums(numbers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_threads_subsums($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speedup and correct result. But not ideal:\n",
    "\n",
    "* cumbersome to do this manually\n",
    "* can have more subtle performance issues like [false sharing](https://en.wikipedia.org/wiki/False_sharing#:~:text=In%20computer%20science%2C%20false%20sharing,managed%20by%20the%20caching%20mechanism.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix 2: Atomics\n",
    "\n",
    "See [Atomic Operations](https://docs.julialang.org/en/v1/manual/multi-threading/#Atomic-Operations) in the Julia doc for more information. But in generaly one shouldn't avoid using them as much as possible since they actually limit the parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage collection\n",
    "\n",
    "[As of now](https://www.youtube.com/watch?v=Ks0p6PQyIPs), **Julia's GC is not parallel** and doesn't work nicely with multithreading.\n",
    "\n",
    "If it gets triggered, it essentially \"stops the world\" (all threads) for clearing up memory.\n",
    "\n",
    "Hence, when using multithreading, it is even more important to **avoid heap allocations!**\n",
    "\n",
    "(If you can't avoid allocations, consider using multiprocessing instead.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level tools for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ThreadsX.jl](https://github.com/tkf/ThreadsX.jl)\n",
    "\n",
    "*Parallelized Base functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ThreadsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThreadsX.sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime ThreadsX.sum($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [FLoops.jl](https://github.com/JuliaFolds/FLoops.jl)\n",
    "\n",
    "*Fast sequential, threaded, and distributed for-loops for Julia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using FLoops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_floops(x)\n",
    "    @floop for xi in x\n",
    "        @reduce(s = zero(eltype(x)) + xi)\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_floops($numbers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = rand(nthreads()*10_000);\n",
    "\n",
    "sum_floops(numbers) ≈ sum(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_serial($numbers);\n",
    "@btime sum_floops($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@floop` supports different *executors* that allow for easy switching between serial and threaded execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_floops(x, executor)\n",
    "    @floop executor for xi in x\n",
    "        @reduce(s += xi)\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_floops($numbers, $(SequentialEx()));\n",
    "@btime sum_floops($numbers, $(ThreadedEx()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more [executors](https://juliafolds.github.io/FLoops.jl/stable/tutorials/parallel/#tutorials-executor), like `DistributedEx` or `CUDAEx`. See, e.g., [FoldsThreads.jl](https://github.com/JuliaFolds/FoldsThreads.jl) and [FoldsCUDA.jl](https://github.com/JuliaFolds/FoldsCUDA.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, FLoops is built on top of [Transducers.jl](https://juliafolds.github.io/Transducers.jl/stable/tutorials/tutorial_parallel/) (i.e. it translates for-loop semantics into folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tullio.jl](https://github.com/mcabbott/Tullio.jl)\n",
    "\n",
    "*Tullio is a very flexible einsum macro* ([Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tullio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(10,10)\n",
    "B = rand(10,10)\n",
    "\n",
    "C = @tullio C[i,j] := A[i,k] * B[k,j] # matrix multiplication\n",
    "\n",
    "C ≈ A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tullio(xs) = @tullio S := xs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_tullio($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Uses `fastmath` and other tricks to be faster here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "\n",
    "*Macro(s) for vectorizing loops.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LoopVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_turbo(x)\n",
    "    s = zero(eltype(x))\n",
    "    @tturbo for i in eachindex(x)\n",
    "        @inbounds s += x[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_turbo($numbers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Uses all kinds of SIMD tricks to be faster than the others.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System topology and thread affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hawk compute node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/lstopo_hawk.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not pinning threads (or pinning them badly) can degrade performance massively!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinning Julia threads to CPU threads\n",
    "\n",
    "What about external tools like `numactl`, `taskset`, etc.? Doesn't work reliably because it [can't distinguish](https://discourse.julialang.org/t/thread-affinitization-pinning-julia-threads-to-cores/58069/5) between Julia threads and other internal threads.\n",
    "\n",
    "**Options:**\n",
    "\n",
    "* Environment variable: `JULIA_EXCLUSIVE=1` (compact pinning)\n",
    "* More control and convenient visualization: [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl)\n",
    "  * `compact`: pin to cpu thread 0, 1, 2, 3, ... one after another\n",
    "  * `spread`: alternate between sockets so, e.g., 0, 64, 1, 65, 2, 66, .... (if a socket has 64 cores)\n",
    "  * `numa`: same as `spread` but alternate between NUMA domains so, e.g., 0, 16, 32, 48, 64, .... (if a NUMA domain has 16 cores)\n",
    "  * **Caveat:** currently one works on Linux.\n",
    "\n",
    "<img src=\"../imgs/threadinfo.png\" width=1000px>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
